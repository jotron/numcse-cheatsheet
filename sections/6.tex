\columnbreak
\part{Approximation in 1D}
\setcounter{section}{0}

%Goal
\textbf{Goal:} For a given function $f$ on $I \subset \R$ find a simpler function $\tilde{f}$ (called \textit{surrogate}) such that the approximation error $\norm{f-\tilde{f}}$ is small for some norm.\\

$\longrightarrow$ Focus on \textbf{Approximation by interpolation}\\
$\longrightarrow$ Hence, good interpolation nodes are key.

\section{Global Polynomial Approx.}

\Def[Polynomial Interp. Approx. scheme] \\
Given a node set $\mathcal{T}=\left\{t_{0}, \ldots, t_{n}\right\} \subset I$, define 
$$\tilde{f} = p \in \mathcal{P}_n \quad \text{s.t } p(t_j)=f(t_j)$$ as the interpolating polynomial.

\sep
\Theorem[Weierstrass] Every continuous function defined on a close interval can be uniformly approximated by a polynomial (of increasing degree). \\

\Theorem[Best approximation, Jackson] For $f \in C^r([-1,1])$ we have
\begin{align*}
	\inf _{p \in \mathcal{P}_{n}}\|f-p\|_{\infty} \leq &  \left(1+\pi^{2} / 2\right)^{r} \frac{(n-r) !}{n !}\left\|f^{(r)}\right\|_{\infty} \\
	\in & \quad \BigO(n^{-r})
\end{align*}

\Lemma[Sensitivity of Interpolation] \\
For data $y$ and noise $\delta$ we have
$$
  \norm{\mathrm{I}_{\mathcal{T}}(y + \delta) - \mathrm{I}_{\mathcal{T}}(y)} = 
  \norm{\mathrm{I}_{\mathcal{T}}(\delta)} \leq
  \norm{\mathrm{I}_{\mathcal{T}}}\norm{\delta}
$$
where $\norm{\mathrm{I}_{\mathcal{T}}}$ denotes the operator norm, for the infinity norm we have
$$
\left\|I_{\mathcal{T}}\right\|_{\infty}:=\sup _{\mathbf{y} \in \mathbb{R}^{n+1}} \frac{\left\|I_{\mathcal{T}}(\mathbf{y})\right\|_{L^{\infty}(I)}}{\|\mathbf{y}\|_{\infty}}=\left\|\sum_{i=0}^{n}\left|L_{i}\right|\right\|_{L^{\infty}(I)}
$$
which coincides with the Lebesgue Constant $\lambda_\mathcal{T}$. \\
This is important because a result for $f\in C^0(I)$ is
$$
\left\|f-\tilde f\right\|_{L^{\infty}(I)} \leq\left(1+\lambda_{\mathcal{T}}\right) \inf _{p \in \mathcal{P}_{n}}\|f-p\|_{L^{\infty}(I)}
$$

\Def[Error convergence] We distinguish
\begin{itemize}
	\item Algebraic convergence: $\norm{f-\tilde{f}}= \BigO(n^{-p})$ with a rate $p>0$
	
	\item Exponential convergence: $\norm{f-\tilde{f}}= \BigO(q^{n})$ for $0<q<1$
\end{itemize}


\sep

\Def[Chebychev interpolation] \\
Polynomial Approximation based on equidistant nodes does not necessarily converge, hence we want a better node set. Based on the error bound 
$$
\norm{f-\tilde f}_{\infty} \leq \frac{1}{(n+1) !}\left\|f^{(n+1)}\right\|_{\infty}\|w\|_{L^{\infty}(I)}
$$
we look for a nodes that minimize the polynomial 
$$
w(t):=\left(t-t_{0}\right) \cdots\left(t-t_{n}\right)
$$
On $I=[-1,1]$ these nodes are the zeros of the $n+1$'th Chebychev Polynomial
$$t_{k}=\cos \left(\frac{2 k+1}{2 n} \pi\right), \quad k=0, \ldots, n-1$$. These nodes are a priori optimal for the approximation error, note that they gather around the borders \\
\textbf{Error Estimate:} $\lambda_\mathcal{T} \in \BigO(\log n)$, hence almost algebraic \\
\textbf{Computational Aspect:} There is a more efficient algorithm to evaluate the resulting interpolating polynomial then standard interpolation algorithms. The key idea is representing $\tilde f$ in Chebychev Polynomial Basis: \\
Evaluation: $\BigO(nm)$ for m points. (Clenshaw Alg.) \\
Comp. of coeffs.: $\BigO(n \log n)$ (FFT Alg.)

\begin{comment}

Smoothness plays a large role to gauge errors

## Finding good interpolation 
-> Chebychev Nodes are a priori optimal
-> Write down the proof of why! (kind of nice) (difference in p_n-1 and n+1 zeros -> ==0)
-> Can be done efficiently by FFT with equidistant nodes?

### 

# What is to take home of "analytic interpolation", just an upper bound on interpolation error?
def analytic ~smooth
\end{comment}

\section{Trigonometric Poly. Approx.}
x \\
x \\
x \\

\begin{comment}
# Trigonometric Approximation (For periodic Signals)

## Error Estimate
-> smoothness leads to faster convergence
-> Every function has a Fourrier Series?

- L2-Norm of Fourrier Series given by sum of coefficients square
- Fourier modes higner than n can be distinguished when only sampling n points, hence they all get mapped to their modulo.
  -> The coeffiences are the sum of all higher multiples.
  
Now to continue on the search for an error estimate, need decay law for Fourrier:
 ->The smoother a periodic function the faster the decay of its Fourier coefficients decay in O(xxx) 6.5.2.20
 
 -> Main Result: algebraic convergenceof theL2-norm of the trigonometric interpolation error forfunctions withlimited smoothness
 
 ## Analytic Functions
 -> exponential convergence
 -> exponential decay of Fourrier coeffs.
 -> The larger the strip of analiticity the beter.
 
 Seems important, why fourrier coefficient equal integral over 0, 1? 4.2.6.20 -> its simply the projection of the function on that vector!
 Write entry in summary about fourrier Series!
\end{comment}


\section{Piecewise Polynomial Approx.}

\Method[Piecewise Lagrange] \\
Given an interval $[a,b]$ endowed with a mesh $\mathcal{M}$, choose local degree and local interpolation nodes for each mesh cell to perform Lagrange Interpolation. \\
\textbf{Error Estimate:} For constant polynomial degree $n$ and mesh width $h_\mathcal{M}$ we have
$$\norm{f-s}_\infty \leqslant \frac{h_{\mathcal{M}}^{n+1}}{(n+1) !}\norm{f^{(n+1)}}_\infty$$ 


\Method[Piecewise Cubic Hermite] \\
Given $f, \mathcal{M}$, the piecewise cubic hermite interpolant is defined as (Note that we use exact slopes!)
$$
s_{\mid\left[x_{j-1}, x_{j}\right]} \in \mathcal{P}_{3}, \quad s\left(x_{j}\right)=f\left(x_{j}\right), \quad s^{\prime}\left(x_{j}\right)=f^{\prime}\left(x_{j}\right)
$$ 

\begin{comment}
 
 ## Spline
  - Convergence in O(h^4)
 Spline vs Hermite?
 
 # Do a subsection on Best approximation Error
 algebraic for limited smoothness
 exponential convergence for analytic functions

6.2.2.15 give it more merit

Remaining: 
- Pullback 

## Error bounds
chebychev bound on analytic functions convergence?

\end{comment}


 
 
